{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "import keras\n",
    "from keras_preprocessing import sequence\n",
    "from nltk import SnowballStemmer, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "train_df = pd.read_csv('/Users/ben/Dropbox/git/kaggle/Sentiment/train.tsv', header=0, delimiter='\\t', quoting=3)\n",
    "test_df = pd.read_csv('/Users/ben/Dropbox/git/kaggle/Sentiment/test.tsv', header=0, delimiter='\\t', quoting=3)\n",
    "raw_docs_train = train_df['Phrase'].values\n",
    "raw_docs_test = test_df['Phrase'].values\n",
    "sentiment_train = train_df['Sentiment'].values\n",
    "num_labels = len(np.unique(sentiment_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean data , convert words to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1422/156060 [00:00<00:21, 7105.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156060/156060 [00:24<00:00, 6485.62it/s]\n",
      "100%|██████████| 66292/66292 [00:09<00:00, 6722.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# used to remove stopwords like 'i','this' 'and' 'of' and so on\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "# stemmer convert words to plain type eg. convert doing -> do , likes -> like\n",
    "stemmer = SnowballStemmer('english')\n",
    "print('pre process')\n",
    "processed_docs_train = []\n",
    "\n",
    "def pre_process(docs):\n",
    "    doc_list = []\n",
    "    # tqdm is a good process status tool \n",
    "    for doc in tqdm(docs):\n",
    "        tokens = word_tokenize(doc)\n",
    "        filtered = [w for w in tokens if w not in stop_words]\n",
    "        stemmed = [stemmer.stem(w) for w in filtered]\n",
    "        doc_list.append(stemmed)\n",
    "    return doc_list\n",
    "\n",
    "\n",
    "train = pre_process(raw_docs_train)\n",
    "test = pre_process(raw_docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156060/156060 [00:00<00:00, 502737.78it/s]\n",
      "100%|██████████| 66292/66292 [00:00<00:00, 358796.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# concat train dataset and test dataset \n",
    "processed_docs_all = np.concatenate((train, test), axis=0)\n",
    "# get word to index dictionary\n",
    "dictionary = corpora.Dictionary(processed_docs_all)\n",
    "\n",
    "dictionary.doc2bow(['cow'])\n",
    "word_len=[]\n",
    "# convert word to vector \n",
    "def wrod2vector(text):\n",
    "    wrod_id = []\n",
    "    for doc in tqdm(text):\n",
    "        word_ids = [dictionary.token2id[w] for w in doc]\n",
    "        wrod_id.append(word_ids)\n",
    "        word_len.append(len(word_ids))\n",
    "    return wrod_id\n",
    "\n",
    "train_ids = wrod2vector(train)\n",
    "test_ids = wrod2vector(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get maxmum sequence length\n",
    "seq_len = np.round((np.mean(word_len) + 3 * np.std(word_len))).astype(int)\n",
    "# trim sequence to seq_len\n",
    "word_id_train = sequence.pad_sequences(np.array(train_ids), maxlen=seq_len)\n",
    "word_id_test = sequence.pad_sequences(np.array(test_ids), maxlen=seq_len)\n",
    "y_train_enc = np_utils.to_categorical(sentiment_train, num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_vector_trim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-755e45480833>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_vector_trim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_vector_trim' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"fitting LSTM ...\")\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(dictionary.keys()), 128, dropout=0.2))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(word_id_train, y_train_enc, nb_epoch=2, batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "test_result = model.predict_classes(test_vecotor_trim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
